# GMVAE_DRIT
Leggere e capire bene cosa fare, scrivere cosa fare cosa capito, leggere due codici separati, pensare alla soluzione come integrare x iscritto e capire come matchare le dimensioni)
Estensione modelli esistenti al fine di armonizzare le immagini (modificare un’immagine composta da più parti, in modo tale che esse risultino coerenti tra loro) mediante un diffusion model. In particolare, applicheremo l’image-to-image translation utilizzando i diffusion models, applicando modelli generativi per trasformare un'immagine di input in un'immagine di output che conserva alcuni attributi originali ma cambia altri secondo la richiesta. In particolare, il modello scelto è il DRIT++.
Il DRIT++ si occupa di separare il contenuto (elementi dell’immagine comuni ad altre immagini) e l’attributo (elementi dell’immagine che permettono di classificare l’immagine) di un’immagine in due domini distinti. 
Cambiare il modo in cui viene mappato l'autoencoder, utilizzando il GMM.
Nella repository DRIT dovremmo cambiare il file model.py e network.py  integrando le parti Inference Nework e Generative Network e GM VAE del notebook python, modificando poi infine le loss nella cartella drit con quelle di GMVAE (aggiungendo pezzi).
1.	Far funzionare la repository e risolvere il problema di nvidia
2.	Forkare drit su github
3.	In drit, lo spazio latente ha distribuzione normale (networks.py, linea 454 e model.py, linea 57 inizializza i pesi come gaussiani). Modificare l'encoder del VAE in modo che, invece di prevedere una singola distribuzione gaussiana (\mu, \sigma^2), il modello preveda i parametri di un GMM, cioè le probabilità dei componenti (\pi), le medie (\mu_k) e le varianze (\sigma_k^2) per ciascun componente del mix.
4.	Sampling dallo spazio latente. Invece di campionare semplicemente da una normale distribuzione gaussiana, eseguire il sampling da una delle componenti del GMM, usando le probabilità (\pi) per scegliere quale componente utilizzare. Networks.py, linea 22: cambiare la definizone di forward con quella il gmvae (ce ne sono un po', capire quale usare) (non sono sicura di questa affermazione)
5.	Modificare encoder con InferenceNet (Inferisce le distribuzioni latenti q(y|x) e q(z_s|y,x)) e decoder con GenerativeNet (Genera la distribuzione latente p(z|y) e ricostruisce l'immagine p(x|z))
6.	Modificare le loss: la funzione di costo del VAE dovrà essere adattata per tenere conto della probabilità del GMM nello spazio latente, usando una combinazione di log-likelihood del GMM (confronto tra x e x_recon) e una term di Kullback-Leibler (KL) che regola la distanza tra la distribuzione latente predetta e il GMM (KL Divergence tra q(z_s|y,x) e p(z|y)) e la distanza tra la distribuzione della variabile categorica y e il GMM (distribuzione desiderata) (KL Divergence tra q(y|x) e una prior p(y)).
7.	Una volta che hai campionato un vettore latente (z_c, z_s), passare al decoder di DRIT++ per generare l'immagine tradotta.
